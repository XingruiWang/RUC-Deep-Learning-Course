{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw7.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNDvFvjzBT3IAmHA3f3lfPV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/XingruiWang/RUC-Deep-Learning-Course/blob/master/Homework/HW_7/hw7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLF77xGY0xF4"
      },
      "source": [
        "# 广告效果预测"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rbb-7m371DCc"
      },
      "source": [
        "### 背景介绍\n",
        "\n",
        "**搜索引擎营销**（SEM: Search Engine Marketing）的核心是通过搜索引擎上，用户主动输入表达的意愿，推送相应的广告信息。搜索引擎营销运营的关键环节之一就是**关键词拓展**。所谓关键词拓展，就是根据已有的关键词表现数据，推测还有哪些尚未购买的关键词。因此，你需要构造一个从**X = 关键词**到**Y = 广告效果**的回归模型。不同场景下对广告效果的定义个不相同。可能是展现、点击、转化、再购买等。这样就构成了一个完整的销售漏斗。而本案例关注该销售漏斗的第一个环节：展现。具体而言，**因变量是Y=log(展现量)**，直接反映了广告主信息在用户面前暴露的强度。而X就是关键词本身的文本信息。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_J4OKjCx1iUB"
      },
      "source": [
        "### 方法介绍\n",
        "\n",
        "#### 一、分词\n",
        "\n",
        "源数据中x为用户搜索内容，为了更好提取内容的语义信息，我们先将内容分割成不同的汉语词语。分词的工具采用的是python中的Jiba包。\n",
        "\n",
        "#### 二、word to vector\n",
        "\n",
        "在分词之后，我使用word-to-vector词编码模型对文本数据进行量化\n",
        "\n",
        "#### 三、RNN模型\n",
        "\n",
        "<img src = 'https://camo.githubusercontent.com/bee147253f0a81a87f02200007716adaf97943f6/687474703a2f2f6b657875652e666d2f7573722f75706c6f6164732f323031352f30382f323036373734313235372e706e67' width=70%>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GMHbwUUI0pRW",
        "outputId": "760921ca-7eb1-4243-bda7-68ae391e9a92"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nH2r8qq4BmHj"
      },
      "source": [
        "### 模型搭建"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vo-cYG3PB0N3"
      },
      "source": [
        "一、数据读入与初步分词"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "id": "JM_4mDwxCsA3",
        "outputId": "0a6862c9-d4c1-4cc2-9e24-1206d9505df0"
      },
      "source": [
        "# drive/MyDrive/RUC/DeepLearning/course7/SEM.csv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import jieba\n",
        "import random\n",
        "from collections import Counter\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "data_table = pd.read_csv('drive/MyDrive/RUC/DeepLearning/course7/SEM.csv')\n",
        "sentences, y = data_table[\"kw\"], data_table[\"logImp\"].to_numpy().astype(np.float32)\n",
        "N = len(y)\n",
        "\n",
        "# 数据预处理\n",
        "y = (y - y.min()) / (y.max() - y.min())\n",
        "train_data = []\n",
        "for s in sentences:\n",
        "  train_data.append(jieba.lcut(s))\n",
        "\n",
        "# train_data = np.array(train_data)\n",
        "\n",
        "print(\"-------------- y的分布 -----------------\")\n",
        "plt.hist(y, bins=50, color = \"#0ABAB5\", alpha = 0.6)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "print(\"\\n-------------- 分词结果 -----------------\")\n",
        "show_index = random.sample(range(N), 10)\n",
        "for s, cut in zip(sentences[show_index], np.array(train_data)[show_index]):\n",
        "  print(s, \"\\t\", cut)\n",
        "\n",
        "\n",
        "print(\"\\n-------------- 词频前20统计 -----------------\")\n",
        "\n",
        "all_words = np.hstack(train_data)\n",
        "words_count = Counter(all_words)\n",
        "W = len(words_count)\n",
        "print(\"%d words in total\"%(W))\n",
        "print(words_count.most_common(20))\n"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------------- y的分布 -----------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "-------------- 分词结果 -----------------\n",
            "飞机票哪个网最便宜 \t ['飞机票', '哪个', '网', '最', '便宜']\n",
            "酷讯网机票查询 \t ['酷讯网', '机票', '查询']\n",
            "飞机票哪里买最便宜 \t ['飞机票', '哪里', '买', '最', '便宜']\n",
            "三亚便宜机票 \t ['三亚', '便宜', '机票']\n",
            "打折机票攻略 \t ['打折', '机票', '攻略']\n",
            "西安的机票 \t ['西安', '的', '机票']\n",
            "郑州便宜机票 \t ['郑州', '便宜', '机票']\n",
            "预订特价北京飞机票 \t ['预订', '特价', '北京', '飞机票']\n",
            "昆明特价机票查询 \t ['昆明', '特价机票', '查询']\n",
            "深圳国内机票预订 \t ['深圳', '国内机票', '预订']\n",
            "\n",
            "-------------- 词频前20统计 -----------------\n",
            "506 words in total\n",
            "[('机票', 1954), ('飞机票', 971), ('特价机票', 633), ('查询', 625), ('预订', 449), ('便宜', 392), ('-', 369), ('打折', 327), ('特价', 268), ('到', 267), ('深圳', 265), ('北京', 256), ('航班', 243), ('广州', 240), ('的', 235), ('上海', 223), ('预定', 215), ('网站', 215), ('网', 180), ('飞机', 170)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80J8crWTKrIg"
      },
      "source": [
        "#### 二、Word to vector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p98idB19B1ez",
        "outputId": "754cc334-c97d-4573-8b4f-7ab3f8821603"
      },
      "source": [
        "# 训练词向量\n",
        "from gensim.models import Word2Vec\n",
        "vec_size = 100\n",
        "\n",
        "w2v = Word2Vec(train_data, size = vec_size, min_count=1)\n",
        "w2v.wv['机票'].shape"
      ],
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 182
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88fEfI6QcLse",
        "outputId": "cbda96c0-f6f2-4df7-e915-b9484cead8a3"
      },
      "source": [
        "# 词编码\n",
        "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer(filters = \"\", split = \" \")\n",
        "tokenizer.fit_on_texts(train_data)\n",
        "\n",
        "train_tokens = tokenizer.texts_to_sequences(train_data)\n",
        "\n",
        "max_len = max([len(t) for t in train_tokens])\n",
        "print(max_len)\n",
        "print(\"----------Tokenizer 结果-----------------\")\n",
        "for d, t in zip(train_data[:10], train_tokens[:10]):\n",
        "  print(d, t)"
      ],
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7\n",
            "----------Tokenizer 结果-----------------\n",
            "['1.5', '折', '机票'] [363, 123, 1]\n",
            "['10.1', '机票'] [240, 1]\n",
            "['10.1', '机票', '查询'] [240, 1, 4]\n",
            "['10.1', '特价机票'] [240, 3]\n",
            "['10', '月份', '特价机票'] [364, 146, 3]\n",
            "['17u', '.', 'cn'] [365, 366, 367]\n",
            "['1', '元', '机票'] [134, 368, 1]\n",
            "['1', '折', '机票', '查询'] [134, 123, 1, 4]\n",
            "['1', '折', '机票', '订购'] [134, 123, 1, 34]\n",
            "['1', '折', '机票网'] [134, 123, 59]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWgo7wkIaDSA"
      },
      "source": [
        "# 序列补全\n",
        "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "train_pad = pad_sequences(train_tokens, maxlen=max_len,\n",
        "                            padding='pre', truncating='pre')"
      ],
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udvUN9EETWzk"
      },
      "source": [
        "# 建立 index --> word 的映射，方便调用\n",
        "reverse_word_map = dict(map(reversed, tokenizer.word_index.items())) # \n",
        "\n",
        "# 用word2vec结果初始化模型Embedding参数\n",
        "# embedding_matrix为一个 [num_words，embedding_dim] 的矩阵\n",
        "# 维度为 507 * 200\n",
        "num_words = W + 1\n",
        "embedding_dim = vec_size\n",
        "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
        "\n",
        "for i in range(1, num_words):\n",
        "    embedding_matrix[i,:] = w2v.wv[reverse_word_map[i]]\n",
        "embedding_matrix = embedding_matrix.astype('float32')"
      ],
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8xI4jD1gjR2"
      },
      "source": [
        "#### 三、用keras搭建RNN模型并训练"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_GV02ERfJKd"
      },
      "source": [
        "# x: train_pad, y: y\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(train_pad,\n",
        "                                                    y,\n",
        "                                                    test_size=0.5,\n",
        "                                                    random_state=0)"
      ],
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQzR1yR0gs5V",
        "outputId": "4240f8be-8a59-47e1-ff86-58d6bc10d686"
      },
      "source": [
        "from keras.layers import Input, Embedding, Dense, SimpleRNN\n",
        "from keras.models import Model\n",
        "\n",
        "# 507 * 200\n",
        "hidden_size = 256\n",
        "\n",
        "def RNN(layers = 2, trainable_embedding = False, embedding_init = [embedding_matrix]):\n",
        "  input = Input(shape = (max_len,))\n",
        "\n",
        "  x = Embedding(num_words, embedding_dim, mask_zero = True, weights=embedding_init,\n",
        "                      trainable = trainable_embedding)(input)\n",
        "  for _ in range(layers - 1):\n",
        "    x = SimpleRNN(hidden_size, return_sequences = True)(x)\n",
        "  x = SimpleRNN(hidden_size, return_sequences = False)(x)\n",
        "  x = Dense(1)(x)\n",
        "  model = Model(input, x)\n",
        "  return model\n",
        "\n",
        "model = RNN(1, False, [embedding_matrix])\n",
        "model.summary()"
      ],
      "execution_count": 191,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_23\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_16 (InputLayer)        [(None, 7)]               0         \n",
            "_________________________________________________________________\n",
            "embedding_14 (Embedding)     (None, 7, 100)            50700     \n",
            "_________________________________________________________________\n",
            "simple_rnn_16 (SimpleRNN)    (None, 256)               91392     \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 142,349\n",
            "Trainable params: 91,649\n",
            "Non-trainable params: 50,700\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RvXq1bc4kNVi",
        "outputId": "0c66ae82-f037-47e0-8cf7-af2db433c81a"
      },
      "source": [
        "model.compile(optimizer=\"Adam\", loss=\"mse\")\n",
        "# model.fit(X_train, y_train)\n",
        "history = model.fit(x=X_train, y=y_train, batch_size=8, epochs=50, verbose=1, validation_data = (X_test, y_test))\n"
      ],
      "execution_count": 192,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "303/303 [==============================] - 2s 7ms/step - loss: 0.0287 - val_loss: 0.0230\n",
            "Epoch 2/50\n",
            "303/303 [==============================] - 2s 7ms/step - loss: 0.0196 - val_loss: 0.0192\n",
            "Epoch 3/50\n",
            "303/303 [==============================] - 2s 6ms/step - loss: 0.0190 - val_loss: 0.0196\n",
            "Epoch 4/50\n",
            "303/303 [==============================] - 2s 7ms/step - loss: 0.0186 - val_loss: 0.0198\n",
            "Epoch 5/50\n",
            "303/303 [==============================] - 2s 6ms/step - loss: 0.0179 - val_loss: 0.0169\n",
            "Epoch 6/50\n",
            "303/303 [==============================] - 2s 6ms/step - loss: 0.0181 - val_loss: 0.0171\n",
            "Epoch 7/50\n",
            "303/303 [==============================] - 2s 7ms/step - loss: 0.0172 - val_loss: 0.0186\n",
            "Epoch 8/50\n",
            "303/303 [==============================] - 2s 6ms/step - loss: 0.0173 - val_loss: 0.0157\n",
            "Epoch 9/50\n",
            "303/303 [==============================] - 2s 6ms/step - loss: 0.0170 - val_loss: 0.0159\n",
            "Epoch 10/50\n",
            "303/303 [==============================] - 2s 6ms/step - loss: 0.0171 - val_loss: 0.0157\n",
            "Epoch 11/50\n",
            "303/303 [==============================] - 2s 6ms/step - loss: 0.0168 - val_loss: 0.0158\n",
            "Epoch 12/50\n",
            "303/303 [==============================] - 2s 6ms/step - loss: 0.0171 - val_loss: 0.0159\n",
            "Epoch 13/50\n",
            "303/303 [==============================] - 2s 6ms/step - loss: 0.0168 - val_loss: 0.0157\n",
            "Epoch 14/50\n",
            "303/303 [==============================] - 2s 6ms/step - loss: 0.0171 - val_loss: 0.0198\n",
            "Epoch 15/50\n",
            "303/303 [==============================] - 2s 6ms/step - loss: 0.0171 - val_loss: 0.0165\n",
            "Epoch 16/50\n",
            "303/303 [==============================] - 2s 6ms/step - loss: 0.0173 - val_loss: 0.0175\n",
            "Epoch 17/50\n",
            "303/303 [==============================] - 2s 6ms/step - loss: 0.0168 - val_loss: 0.0163\n",
            "Epoch 18/50\n",
            "303/303 [==============================] - 2s 6ms/step - loss: 0.0170 - val_loss: 0.0160\n",
            "Epoch 19/50\n",
            "303/303 [==============================] - 2s 7ms/step - loss: 0.0168 - val_loss: 0.0159\n",
            "Epoch 20/50\n",
            "303/303 [==============================] - 2s 6ms/step - loss: 0.0167 - val_loss: 0.0166\n",
            "Epoch 21/50\n",
            "303/303 [==============================] - 2s 6ms/step - loss: 0.0167 - val_loss: 0.0157\n",
            "Epoch 22/50\n",
            "303/303 [==============================] - 2s 6ms/step - loss: 0.0162 - val_loss: 0.0151\n",
            "Epoch 23/50\n",
            "303/303 [==============================] - 2s 7ms/step - loss: 0.0162 - val_loss: 0.0161\n",
            "Epoch 24/50\n",
            "303/303 [==============================] - 2s 6ms/step - loss: 0.0161 - val_loss: 0.0150\n",
            "Epoch 25/50\n",
            "303/303 [==============================] - 2s 6ms/step - loss: 0.0158 - val_loss: 0.0147\n",
            "Epoch 26/50\n",
            "303/303 [==============================] - 2s 7ms/step - loss: 0.0166 - val_loss: 0.0148\n",
            "Epoch 27/50\n",
            "303/303 [==============================] - 2s 6ms/step - loss: 0.0156 - val_loss: 0.0148\n",
            "Epoch 28/50\n",
            "303/303 [==============================] - 2s 6ms/step - loss: 0.0159 - val_loss: 0.0148\n",
            "Epoch 29/50\n",
            "303/303 [==============================] - 2s 7ms/step - loss: 0.0156 - val_loss: 0.0145\n",
            "Epoch 30/50\n",
            "303/303 [==============================] - 2s 6ms/step - loss: 0.0156 - val_loss: 0.0145\n",
            "Epoch 31/50\n",
            "303/303 [==============================] - 2s 7ms/step - loss: 0.0156 - val_loss: 0.0143\n",
            "Epoch 32/50\n",
            "303/303 [==============================] - 2s 7ms/step - loss: 0.0152 - val_loss: 0.0142\n",
            "Epoch 33/50\n",
            "303/303 [==============================] - 2s 7ms/step - loss: 0.0153 - val_loss: 0.0147\n",
            "Epoch 34/50\n",
            "303/303 [==============================] - 2s 7ms/step - loss: 0.0152 - val_loss: 0.0142\n",
            "Epoch 35/50\n",
            "303/303 [==============================] - 2s 7ms/step - loss: 0.0149 - val_loss: 0.0236\n",
            "Epoch 36/50\n",
            "303/303 [==============================] - 2s 7ms/step - loss: 0.0157 - val_loss: 0.0144\n",
            "Epoch 37/50\n",
            "303/303 [==============================] - 2s 7ms/step - loss: 0.0156 - val_loss: 0.0148\n",
            "Epoch 38/50\n",
            "303/303 [==============================] - 2s 7ms/step - loss: 0.0150 - val_loss: 0.0144\n",
            "Epoch 39/50\n",
            "303/303 [==============================] - 2s 7ms/step - loss: 0.0152 - val_loss: 0.0141\n",
            "Epoch 40/50\n",
            "303/303 [==============================] - 2s 6ms/step - loss: 0.0147 - val_loss: 0.0140\n",
            "Epoch 41/50\n",
            "303/303 [==============================] - 2s 6ms/step - loss: 0.0150 - val_loss: 0.0147\n",
            "Epoch 42/50\n",
            "303/303 [==============================] - 2s 6ms/step - loss: 0.0151 - val_loss: 0.0142\n",
            "Epoch 43/50\n",
            "303/303 [==============================] - 2s 6ms/step - loss: 0.0148 - val_loss: 0.0152\n",
            "Epoch 44/50\n",
            "303/303 [==============================] - 2s 6ms/step - loss: 0.0149 - val_loss: 0.0137\n",
            "Epoch 45/50\n",
            "303/303 [==============================] - 2s 7ms/step - loss: 0.0151 - val_loss: 0.0144\n",
            "Epoch 46/50\n",
            "303/303 [==============================] - 2s 6ms/step - loss: 0.0150 - val_loss: 0.0143\n",
            "Epoch 47/50\n",
            "303/303 [==============================] - 2s 6ms/step - loss: 0.0145 - val_loss: 0.0139\n",
            "Epoch 48/50\n",
            "303/303 [==============================] - 2s 6ms/step - loss: 0.0147 - val_loss: 0.0166\n",
            "Epoch 49/50\n",
            "303/303 [==============================] - 2s 6ms/step - loss: 0.0148 - val_loss: 0.0147\n",
            "Epoch 50/50\n",
            "303/303 [==============================] - 2s 6ms/step - loss: 0.0144 - val_loss: 0.0143\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cU9LBuul4n3",
        "outputId": "fd2a22e6-9471-47ec-d98a-1196bad31942"
      },
      "source": [
        "y_pred = []\n",
        "# for x_test in X_test:\n",
        "#   y_pred.append(model.predict(X_test))\n",
        "# y_pred\n",
        "\n",
        "model.predict(X_test, batch_size=8)\n"
      ],
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.44537914],\n",
              "       [0.44535673],\n",
              "       [0.4453562 ],\n",
              "       ...,\n",
              "       [0.44535637],\n",
              "       [0.44537914],\n",
              "       [0.44535732]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 179
        }
      ]
    }
  ]
}